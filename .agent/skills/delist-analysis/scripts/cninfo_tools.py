#!/usr/bin/env python
# -*- coding: UTF-8 -*-
"""
å·¨æ½®èµ„è®¯å·¥å…·é›†

æä¾›ä»¥ä¸‹å‘½ä»¤ï¼š
1. list-announcements - è·å–è‚¡ç¥¨å…¬å‘Šåˆ—è¡¨
2. download-pdf - ä¸‹è½½å…¬å‘ŠPDF
3. extract-text - ä»PDFæå–æ–‡æœ¬
4. append-result - è¿½åŠ ç»“æœåˆ°CSV

ä¾› Agent è°ƒç”¨ï¼Œæ‰§è¡Œæ•°æ®è·å–å’Œå­˜å‚¨ä»»åŠ¡ã€‚
"""

from __future__ import annotations

import argparse
import csv
import json
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
from zoneinfo import ZoneInfo

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry


class CNINFOClient:
    """å·¨æ½®èµ„è®¯APIå®¢æˆ·ç«¯"""

    STOCK_QUERY_URL = "http://www.cninfo.com.cn/new/hisAnnouncement/query"
    STOCK_INFO_URL = "http://www.cninfo.com.cn/new/information/topSearch/query"

    HEADERS = {
        "Accept": "*/*",
        "Accept-Encoding": "gzip, deflate",
        "Accept-Language": "zh-CN,zh;q=0.9",
        "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
        "Host": "www.cninfo.com.cn",
        "Origin": "http://www.cninfo.com.cn",
        "Referer": "http://www.cninfo.com.cn/new/disclosure/stock",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        "X-Requested-With": "XMLHttpRequest"
    }

    def __init__(self, timeout: int = 30, max_retries: int = 3):
        self.timeout = timeout
        self.session = requests.Session()
        self.session.headers.update(self.HEADERS)
        retries = Retry(total=max_retries, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
        self.session.mount('http://', HTTPAdapter(max_retries=retries))
        self.session.mount('https://', HTTPAdapter(max_retries=retries))

    def _get_org_id(self, stock_code: str) -> Optional[str]:
        """è·å–è‚¡ç¥¨çš„orgId"""
        # å…ˆå°è¯•æ„é€ 
        if stock_code.startswith('6'):
            constructed = f"gssh0{stock_code}"
        elif stock_code.startswith('0') or stock_code.startswith('3'):
            constructed = f"gssz0{stock_code}"
        elif stock_code.startswith('8') or stock_code.startswith('4'):
            constructed = f"gsbj0{stock_code}"
        else:
            constructed = f"gssz0{stock_code}"

        # é€šè¿‡APIæŸ¥è¯¢éªŒè¯
        try:
            response = self.session.post(
                self.STOCK_INFO_URL,
                data={"keyWord": stock_code},
                timeout=self.timeout
            )
            response.raise_for_status()
            result = response.json()
            if isinstance(result, list):
                for item in result:
                    if item.get("code") == stock_code:
                        return item.get("orgId", constructed)
        except Exception:
            pass

        return constructed

    def list_announcements(
        self,
        stock_code: str,
        keyword: str = "",
        sort: str = "desc",
        limit: int = 30
    ) -> List[Dict[str, Any]]:
        """
        è·å–è‚¡ç¥¨å…¬å‘Šåˆ—è¡¨
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            keyword: æœç´¢å…³é”®è¯ï¼ˆå¯é€‰ï¼‰
            sort: æ’åºæ–¹å¼ asc/desc
            limit: è¿”å›æ•°é‡é™åˆ¶
            
        Returns:
            å…¬å‘Šåˆ—è¡¨ [{date, title, id, url}, ...]
        """
        org_id = self._get_org_id(stock_code)
        all_announcements = []
        page_num = 1
        page_size = 30

        while len(all_announcements) < limit:
            data = {
                "pageNum": page_num,
                "pageSize": page_size,
                "column": "szse",
                "tabName": "fulltext",
                "stock": f"{stock_code},{org_id}",
                "searchkey": keyword,
                "category": "",
                "seDate": "",
                "sortName": "time",
                "sortType": sort,
                "isHLtitle": "false"
            }

            try:
                response = self.session.post(
                    self.STOCK_QUERY_URL, data=data, timeout=self.timeout
                )
                response.raise_for_status()
                response_data = response.json()
            except Exception as e:
                print(f"Error fetching announcements: {e}", file=sys.stderr)
                break

            announcements = response_data.get("announcements", [])
            if not announcements:
                break

            for ann in announcements:
                if len(all_announcements) >= limit:
                    break
                    
                ann_time = ann.get("announcementTime", 0)
                if ann_time:
                    tz = ZoneInfo("Asia/Shanghai")
                    dt = datetime.fromtimestamp(ann_time / 1000, tz=tz)
                    date_str = dt.strftime("%Y-%m-%d")
                else:
                    date_str = ""

                adjunct_url = ann.get("adjunctUrl", "")
                full_url = f"http://static.cninfo.com.cn/{adjunct_url}" if adjunct_url else ""

                all_announcements.append({
                    "date": date_str,
                    "title": ann.get("announcementTitle", ""),
                    "id": str(ann.get("announcementId", "")),
                    "url": full_url,
                    "secName": ann.get("secName", "")
                })

            if not response_data.get("hasMore", False):
                break

            page_num += 1

        return all_announcements[:limit]

    def download_pdf(self, url: str, output_path: str) -> bool:
        """
        ä¸‹è½½PDFæ–‡ä»¶
        
        Args:
            url: PDFçš„URL
            output_path: ä¿å­˜è·¯å¾„
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            # PDFä¸‹è½½éœ€è¦ä½¿ç”¨ä¸åŒçš„headers
            pdf_headers = {
                "Accept": "application/pdf,*/*",
                "Accept-Encoding": "gzip, deflate",
                "Accept-Language": "zh-CN,zh;q=0.9",
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Referer": "http://www.cninfo.com.cn/new/disclosure/stock",
            }
            
            response = requests.get(url, headers=pdf_headers, timeout=self.timeout, stream=True, allow_redirects=True)
            response.raise_for_status()

            with open(output_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)

            return os.path.getsize(output_path) > 0
        except Exception as e:
            print(f"Error downloading PDF: {e}", file=sys.stderr)
            return False


def extract_text_from_pdf(pdf_path: str, max_pages: int = 10) -> str:
    """
    ä»PDFæå–æ–‡æœ¬
    
    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
        max_pages: æœ€å¤§æå–é¡µæ•°
        
    Returns:
        æå–çš„æ–‡æœ¬å†…å®¹
    """
    try:
        import pdfplumber
        text_parts = []
        with pdfplumber.open(pdf_path) as pdf:
            for i, page in enumerate(pdf.pages[:max_pages]):
                page_text = page.extract_text()
                if page_text:
                    text_parts.append(f"--- Page {i+1} ---\n{page_text}")
        return "\n\n".join(text_parts)
    except ImportError:
        print("Error: pdfplumber not installed. Run: pip install pdfplumber", file=sys.stderr)
        return ""
    except Exception as e:
        print(f"Error extracting text: {e}", file=sys.stderr)
        return ""


def append_result_to_csv(csv_path: str, data: Dict[str, Any]) -> bool:
    """
    è¿½åŠ ç»“æœåˆ°CSVæ–‡ä»¶
    
    Args:
        csv_path: CSVæ–‡ä»¶è·¯å¾„
        data: è¦è¿½åŠ çš„æ•°æ®å­—å…¸
        
    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    headers = [
        "code", "åç§°", "é€€å¸‚æ—¥æœŸ", "é€€å¸‚åŸå› ", "é€€å¸‚ç±»å‹",
        "é¦–æ¬¡é€€å¸‚é€šçŸ¥æ—¥", "åœç‰Œå¼€å§‹æ—¥", "ç½®æ¢æ ‡çš„code", "ç½®æ¢æ ‡çš„åç§°", "ç½®æ¢æ¯”ä¾‹",
        "ç½®æ¢å®Œæˆæ—¥æœŸ", "æ¥æºå…¬å‘Š", "å…¬å‘ŠURL"
    ]

    file_exists = os.path.exists(csv_path)

    try:
        with open(csv_path, 'a', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=headers)
            if not file_exists:
                writer.writeheader()
            
            # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æœ‰å€¼
            row = {h: data.get(h, "NaN") for h in headers}
            writer.writerow(row)
        return True
    except Exception as e:
        print(f"Error appending to CSV: {e}", file=sys.stderr)
        return False


# é€€å¸‚ç±»å‹å®šä¹‰
DELIST_TYPES = {
    "MERGE": "å¸æ”¶åˆå¹¶é€€å¸‚",
    "VOLUNTARY": "ä¸»åŠ¨é€€å¸‚",
    "TENDER": "è¦çº¦æ”¶è´­é€€å¸‚",
    "FORCE_FIN": "å¼ºåˆ¶é€€å¸‚_è´¢åŠ¡",
    "FORCE_TRADE": "å¼ºåˆ¶é€€å¸‚_äº¤æ˜“",
    "FORCE_FRAUD": "å¼ºåˆ¶é€€å¸‚_è¿æ³•",
    "FORCE_NORM": "å¼ºåˆ¶é€€å¸‚_è§„èŒƒ",
    "OTHER": "å…¶ä»–"
}

# éœ€è¦ç½®æ¢å­—æ®µçš„ç±»å‹
TYPES_REQUIRE_SWAP = {"MERGE"}
# å¯èƒ½éœ€è¦ç½®æ¢å­—æ®µçš„ç±»å‹ï¼ˆè‚¡ç¥¨è¦çº¦ï¼‰
TYPES_MAYBE_SWAP = {"TENDER"}
# ä¸éœ€è¦ç½®æ¢å­—æ®µçš„ç±»å‹
TYPES_NO_SWAP = {"VOLUNTARY", "FORCE_FIN", "FORCE_TRADE", "FORCE_FRAUD", "FORCE_NORM", "OTHER"}

# ç½®æ¢ç›¸å…³å­—æ®µ
SWAP_FIELDS = ["ç½®æ¢æ ‡çš„code", "ç½®æ¢æ ‡çš„åç§°", "ç½®æ¢æ¯”ä¾‹", "ç½®æ¢å®Œæˆæ—¥æœŸ"]
# é€šç”¨å¿…å¡«å­—æ®µ - æ‰€æœ‰é€€å¸‚ç±»å‹éƒ½å¿…é¡»æœ‰è¿™äº›å­—æ®µ
REQUIRED_FIELDS = [
    "code", "åç§°", "é€€å¸‚æ—¥æœŸ", "é€€å¸‚åŸå› ", "é€€å¸‚ç±»å‹", 
    "é¦–æ¬¡é€€å¸‚é€šçŸ¥æ—¥",  # PITå…³é”®ï¼šæŠ•èµ„è€…é¦–æ¬¡è·çŸ¥
    "åœç‰Œå¼€å§‹æ—¥",       # PITå…³é”®ï¼šæœ€åå–å‡ºæœºä¼š
    "æ¥æºå…¬å‘Š", "å…¬å‘ŠURL"
]


def validate_result(data: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ ¡éªŒæå–ç»“æœ
    
    Args:
        data: æå–çš„æ•°æ®å­—å…¸
        
    Returns:
        æ ¡éªŒç»“æœ {"valid": bool, "errors": [...], "warnings": [...]}
    """
    errors = []
    warnings = []
    
    # 1. æ£€æŸ¥é€šç”¨å¿…å¡«å­—æ®µ
    for field in REQUIRED_FIELDS:
        value = data.get(field, "")
        if not value or value == "NaN":
            errors.append({
                "type": "MISSING_REQUIRED",
                "field": field,
                "message": f"å¿…å¡«å­—æ®µ '{field}' ç¼ºå¤±æˆ–ä¸ºç©º"
            })
    
    # 2. æ£€æŸ¥ code æ ¼å¼ (å¢å¼ºç‰ˆï¼šå¿…é¡»æ˜¯6ä½æ•°å­—å­—ç¬¦ä¸²)
    code = data.get("code", "")
    if code and code != "NaN":
        # æ£€æŸ¥ç±»å‹å¿…é¡»æ˜¯å­—ç¬¦ä¸²
        if not isinstance(code, str):
            errors.append({
                "type": "INVALID_FORMAT",
                "field": "code",
                "message": f"è‚¡ç¥¨ä»£ç å¿…é¡»æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œå½“å‰ä¸º {type(code).__name__}: {code}ã€‚"
                           f"è¯·ä½¿ç”¨å¼•å·åŒ…è£¹ï¼Œå¦‚ \"000001\" è€Œé 1"
            })
        # æ£€æŸ¥é•¿åº¦å’Œæ ¼å¼
        elif not (len(code) == 6 and code.isdigit()):
            errors.append({
                "type": "INVALID_FORMAT",
                "field": "code",
                "message": f"è‚¡ç¥¨ä»£ç æ ¼å¼é”™è¯¯: '{code}'ï¼Œåº”ä¸º6ä½æ•°å­—å­—ç¬¦ä¸²ï¼ˆå¦‚ '000001'ï¼‰"
            })
    
    # 3. æ£€æŸ¥æ—¥æœŸæ ¼å¼
    date_fields = ["é€€å¸‚æ—¥æœŸ", "é¦–æ¬¡é€€å¸‚é€šçŸ¥æ—¥", "åœç‰Œå¼€å§‹æ—¥", "ç½®æ¢å®Œæˆæ—¥æœŸ"]
    for field in date_fields:
        value = data.get(field, "")
        if value and value != "NaN":
            try:
                datetime.strptime(value, "%Y-%m-%d")
            except ValueError:
                errors.append({
                    "type": "INVALID_FORMAT",
                    "field": field,
                    "message": f"æ—¥æœŸæ ¼å¼é”™è¯¯: '{value}'ï¼Œåº”ä¸º YYYY-MM-DD"
                })
    
    # 4. æ£€æŸ¥æ—¥æœŸé€»è¾‘
    first_notice = data.get("é¦–æ¬¡é€€å¸‚é€šçŸ¥æ—¥", "")
    suspend_date = data.get("åœç‰Œå¼€å§‹æ—¥", "")
    delist_date = data.get("é€€å¸‚æ—¥æœŸ", "")
    
    # 4.1 é¦–æ¬¡é€šçŸ¥æ—¥ < é€€å¸‚æ—¥æœŸ
    if first_notice and delist_date and first_notice != "NaN" and delist_date != "NaN":
        try:
            d1 = datetime.strptime(first_notice, "%Y-%m-%d")
            d2 = datetime.strptime(delist_date, "%Y-%m-%d")
            if d1 >= d2:
                errors.append({
                    "type": "LOGIC_ERROR",
                    "field": "é¦–æ¬¡é€€å¸‚é€šçŸ¥æ—¥",
                    "message": f"é¦–æ¬¡é€€å¸‚é€šçŸ¥æ—¥({first_notice})åº”æ—©äºé€€å¸‚æ—¥æœŸ({delist_date})"
                })
        except ValueError:
            pass
    
    # 4.2 é¦–æ¬¡é€šçŸ¥æ—¥ <= åœç‰Œå¼€å§‹æ—¥ <= é€€å¸‚æ—¥æœŸ
    if suspend_date and suspend_date != "NaN":
        try:
            d_suspend = datetime.strptime(suspend_date, "%Y-%m-%d")
            if first_notice and first_notice != "NaN":
                d_notice = datetime.strptime(first_notice, "%Y-%m-%d")
                if d_suspend < d_notice:
                    errors.append({
                        "type": "LOGIC_ERROR",
                        "field": "åœç‰Œå¼€å§‹æ—¥",
                        "message": f"åœç‰Œå¼€å§‹æ—¥({suspend_date})åº”æ™šäºæˆ–ç­‰äºé¦–æ¬¡é€€å¸‚é€šçŸ¥æ—¥({first_notice})"
                    })
                    
                # 4.3 åˆç†æ€§æ£€éªŒï¼šé¦–æ¬¡é€šçŸ¥æ—¥ä¸åœç‰Œæ—¥é—´éš”åº”è¶³å¤Ÿé•¿
                days_gap = (d_suspend - d_notice).days
                if days_gap < 7:  # é—´éš”å°äº7å¤©ä¸ºè­¦å‘Š
                    warnings.append({
                        "type": "SHORT_INTERVAL",
                        "message": f"é¦–æ¬¡é€€å¸‚é€šçŸ¥æ—¥({first_notice})ä¸åœç‰Œå¼€å§‹æ—¥({suspend_date})ä»…ç›¸éš”{days_gap}å¤©ï¼Œ"
                                   f"æŠ•èµ„è€…ååº”æ—¶é—´å¾ˆçŸ­ã€‚è¯·ç¡®è®¤é¦–æ¬¡é€šçŸ¥æ—¥æ˜¯å¦æ­£ç¡®ï¼Œå¯èƒ½éœ€è¦æœç´¢æ›´æ—©çš„å…¬å‘Šï¼ˆå¦‚'ç­¹åˆ’é‡ç»„'ã€'ç­¹åˆ’é‡å¤§äº‹é¡¹'ç­‰ï¼‰"
                    })
                    
            if delist_date and delist_date != "NaN":
                d_delist = datetime.strptime(delist_date, "%Y-%m-%d")
                if d_suspend > d_delist:
                    errors.append({
                        "type": "LOGIC_ERROR",
                        "field": "åœç‰Œå¼€å§‹æ—¥",
                        "message": f"åœç‰Œå¼€å§‹æ—¥({suspend_date})åº”æ—©äºæˆ–ç­‰äºé€€å¸‚æ—¥æœŸ({delist_date})"
                    })
        except ValueError:
            pass
    
    # 5. æ£€æŸ¥é€€å¸‚ç±»å‹
    delist_type = data.get("é€€å¸‚ç±»å‹", "")
    if delist_type and delist_type != "NaN":
        if delist_type not in DELIST_TYPES:
            errors.append({
                "type": "UNKNOWN_TYPE",
                "field": "é€€å¸‚ç±»å‹",
                "message": f"æœªçŸ¥çš„é€€å¸‚ç±»å‹: '{delist_type}'ï¼Œæœ‰æ•ˆå€¼: {list(DELIST_TYPES.keys())}"
            })
    
    # 6. æ£€æŸ¥åˆ†ç±»å­—æ®µä¸€è‡´æ€§
    if delist_type in TYPES_REQUIRE_SWAP:
        # MERGE ç±»å‹å¿…é¡»æœ‰ç½®æ¢ä¿¡æ¯
        for field in SWAP_FIELDS:
            value = data.get(field, "NaN")
            if not value or value == "NaN":
                errors.append({
                    "type": "FIELD_CONFLICT",
                    "field": field,
                    "message": f"é€€å¸‚ç±»å‹ä¸º {delist_type}ï¼Œå­—æ®µ '{field}' å¿…é¡»æœ‰å€¼ï¼ˆå½“å‰ä¸º NaNï¼‰"
                })
        
        # æ£€æŸ¥ç½®æ¢æ¯”ä¾‹æ ¼å¼
        ratio = data.get("ç½®æ¢æ¯”ä¾‹", "")
        if ratio and ratio != "NaN":
            import re
            if not re.match(r'^\d+:\d+\.?\d*$', ratio):
                errors.append({
                    "type": "INVALID_FORMAT",
                    "field": "ç½®æ¢æ¯”ä¾‹",
                    "message": f"ç½®æ¢æ¯”ä¾‹æ ¼å¼é”™è¯¯: '{ratio}'ï¼Œåº”ä¸º '1:X.XXXX' æ ¼å¼"
                })
        
        # æ£€æŸ¥ç½®æ¢æ ‡çš„codeæ ¼å¼ (å¢å¼ºç‰ˆ)
        target_code = data.get("ç½®æ¢æ ‡çš„code", "")
        if target_code and target_code != "NaN":
            if not isinstance(target_code, str):
                errors.append({
                    "type": "INVALID_FORMAT",
                    "field": "ç½®æ¢æ ‡çš„code",
                    "message": f"ç½®æ¢æ ‡çš„codeå¿…é¡»æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œå½“å‰ä¸º {type(target_code).__name__}: {target_code}"
                })
            elif not (len(target_code) == 6 and target_code.isdigit()):
                errors.append({
                    "type": "INVALID_FORMAT",
                    "field": "ç½®æ¢æ ‡çš„code",
                    "message": f"ç½®æ¢æ ‡çš„ä»£ç æ ¼å¼é”™è¯¯: '{target_code}'ï¼Œåº”ä¸º6ä½æ•°å­—"
                })
    
    elif delist_type in TYPES_NO_SWAP:
        # éåˆå¹¶ç±»å‹ï¼Œç½®æ¢å­—æ®µå¿…é¡»ä¸º NaN
        for field in SWAP_FIELDS:
            value = data.get(field, "NaN")
            if value and value != "NaN":
                errors.append({
                    "type": "FIELD_CONFLICT",
                    "field": field,
                    "message": f"é€€å¸‚ç±»å‹ä¸º {delist_type}ï¼Œå­—æ®µ '{field}' åº”ä¸º NaNï¼ˆå½“å‰ä¸º '{value}'ï¼‰"
                })
    
    elif delist_type in TYPES_MAYBE_SWAP:
        # TENDER ç±»å‹å¯èƒ½æœ‰ä¹Ÿå¯èƒ½æ²¡æœ‰ç½®æ¢
        swap_values = [data.get(f, "NaN") for f in SWAP_FIELDS]
        has_any = any(v and v != "NaN" for v in swap_values)
        has_all = all(v and v != "NaN" for v in swap_values)
        
        if has_any and not has_all:
            warnings.append({
                "type": "PARTIAL_SWAP",
                "message": "è¦çº¦æ”¶è´­é€€å¸‚çš„ç½®æ¢å­—æ®µä¸å®Œæ•´ï¼Œè¯·ç¡®è®¤æ˜¯ç°é‡‘è¦çº¦è¿˜æ˜¯è‚¡ç¥¨è¦çº¦"
            })
    
    # 7. æ£€æŸ¥ URL æ ¼å¼
    url = data.get("å…¬å‘ŠURL", "")
    if url and url != "NaN":
        if not url.startswith("http"):
            errors.append({
                "type": "INVALID_FORMAT",
                "field": "å…¬å‘ŠURL",
                "message": f"URLæ ¼å¼é”™è¯¯: '{url}'ï¼Œåº”ä»¥ http å¼€å¤´"
            })
    
    return {
        "valid": len(errors) == 0,
        "errors": errors,
        "warnings": warnings,
        "data": data
    }


# é£é™©ä¿¡å·å…³é”®è¯
RISK_KEYWORDS = {
    "CRITICAL": [  # ğŸ”´ ç´§æ€¥
        "è‚¡ä¸œå¤§ä¼šå†³è®®æ–¹å¼ä¸»åŠ¨ç»ˆæ­¢",
        "è‚¡ä¸œå¤§ä¼šé€šè¿‡.*å¸æ”¶åˆå¹¶",
        "ç»ˆæ­¢ä¸Šå¸‚çš„å†³å®š",
        "ç»ˆæ­¢ä¸Šå¸‚æš¨æ‘˜ç‰Œ",
        "è‚¡ç¥¨ç»ˆæ­¢ä¸Šå¸‚çš„å…¬å‘Š",
        "åœç‰Œå…¬å‘Š.*ç»ˆæ­¢ä¸Šå¸‚",
    ],
    "HIGH": [  # ğŸŸ  é«˜é£é™©
        "æ¢è‚¡å¸æ”¶åˆå¹¶.*é¢„æ¡ˆ",
        "å¸æ”¶åˆå¹¶.*é¢„æ¡ˆ",
        "ä¸»åŠ¨ç»ˆæ­¢ä¸Šå¸‚.*é¢„æ¡ˆ",
        "æ”¶åˆ°.*äº‹å…ˆå‘ŠçŸ¥ä¹¦",
        "è‘£äº‹ä¼š.*é€šè¿‡.*åˆå¹¶",
    ],
    "MEDIUM": [  # ğŸŸ¡ ä¸­é£é™©
        "è§¦å‘é€€å¸‚æ¡ä»¶",
        "å¯èƒ½ç»ˆæ­¢ä¸Šå¸‚çš„é£é™©æç¤º",
        "è¿ç»­äºæŸ",
        "å‡€èµ„äº§ä¸ºè´Ÿ",
        "æ”¶åˆ°ç»ˆæ­¢ä¸Šå¸‚.*äº‹å…ˆå‘ŠçŸ¥ä¹¦",
    ],
    "LOW": [  # ğŸŸ¢ ä½é£é™©
        "ç­¹åˆ’é‡å¤§èµ„äº§é‡ç»„",
        "ç­¹åˆ’é‡å¤§äº‹é¡¹",
        "é‡å¤§èµ„äº§é‡ç»„åœç‰Œ",
    ]
}


def scan_delist_risk(announcements: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    æ‰«æå…¬å‘Šåˆ—è¡¨ï¼Œæ£€æµ‹é€€å¸‚é£é™©ä¿¡å·
    
    Args:
        announcements: å…¬å‘Šåˆ—è¡¨
        
    Returns:
        é£é™©æ‰«æç»“æœ {"risk_level": str, "signals": [...]}
    """
    import re
    
    signals = []
    highest_level = None
    level_priority = {"CRITICAL": 0, "HIGH": 1, "MEDIUM": 2, "LOW": 3}
    
    for ann in announcements:
        title = ann.get("title", "")
        date = ann.get("date", "")
        
        for level, keywords in RISK_KEYWORDS.items():
            for keyword in keywords:
                if re.search(keyword, title):
                    signals.append({
                        "level": level,
                        "date": date,
                        "title": title,
                        "keyword": keyword,
                        "url": ann.get("url", "")
                    })
                    
                    # æ›´æ–°æœ€é«˜é£é™©ç­‰çº§
                    if highest_level is None or level_priority[level] < level_priority[highest_level]:
                        highest_level = level
                    break  # ä¸€ä¸ªå…¬å‘ŠåªåŒ¹é…ä¸€ä¸ªç­‰çº§
    
    # æŒ‰é£é™©ç­‰çº§å’Œæ—¥æœŸæ’åº
    signals.sort(key=lambda x: (level_priority.get(x["level"], 99), x["date"]))
    
    return {
        "risk_level": highest_level or "NONE",
        "signal_count": len(signals),
        "signals": signals
    }


def main():
    parser = argparse.ArgumentParser(description="å·¨æ½®èµ„è®¯å·¥å…·é›†")
    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")

    # list-announcements
    list_parser = subparsers.add_parser("list-announcements", help="è·å–è‚¡ç¥¨å…¬å‘Šåˆ—è¡¨")
    list_parser.add_argument("stock_code", help="è‚¡ç¥¨ä»£ç ")
    list_parser.add_argument("--keyword", "-k", default="", help="æœç´¢å…³é”®è¯")
    list_parser.add_argument("--sort", "-s", choices=["asc", "desc"], default="desc", help="æ’åºæ–¹å¼")
    list_parser.add_argument("--limit", "-l", type=int, default=30, help="è¿”å›æ•°é‡é™åˆ¶")

    # download-pdf
    dl_parser = subparsers.add_parser("download-pdf", help="ä¸‹è½½å…¬å‘ŠPDF")
    dl_parser.add_argument("url", help="PDFçš„URL")
    dl_parser.add_argument("--output", "-o", required=True, help="ä¿å­˜è·¯å¾„")

    # extract-text
    ext_parser = subparsers.add_parser("extract-text", help="ä»PDFæå–æ–‡æœ¬")
    ext_parser.add_argument("pdf_path", help="PDFæ–‡ä»¶è·¯å¾„")
    ext_parser.add_argument("--max-pages", "-m", type=int, default=10, help="æœ€å¤§æå–é¡µæ•°")

    # append-result
    app_parser = subparsers.add_parser("append-result", help="è¿½åŠ ç»“æœåˆ°CSV")
    app_parser.add_argument("--csv", "-c", required=True, help="CSVæ–‡ä»¶è·¯å¾„")
    app_group = app_parser.add_mutually_exclusive_group(required=True)
    app_group.add_argument("--data", "-d", help="JSONæ ¼å¼çš„æ•°æ®")
    app_group.add_argument("--file", "-f", help="åŒ…å«JSONæ•°æ®çš„æ–‡ä»¶è·¯å¾„")

    # validate
    val_parser = subparsers.add_parser("validate", help="æ ¡éªŒæå–ç»“æœ")
    val_group = val_parser.add_mutually_exclusive_group(required=True)
    val_group.add_argument("--data", "-d", help="JSONæ ¼å¼çš„æ•°æ®")
    val_group.add_argument("--file", "-f", help="åŒ…å«JSONæ•°æ®çš„æ–‡ä»¶è·¯å¾„")

    # scan-risk
    scan_parser = subparsers.add_parser("scan-risk", help="æ‰«æè‚¡ç¥¨é€€å¸‚é£é™©")
    scan_parser.add_argument("stock_code", help="è‚¡ç¥¨ä»£ç ")
    scan_parser.add_argument("--days", "-d", type=int, default=30, help="æ‰«ææœ€è¿‘Nå¤©çš„å…¬å‘Š")

    # filter-delist: ç­›é€‰é€€å¸‚ç›¸å…³å…¬å‘Š
    filter_parser = subparsers.add_parser("filter-delist", help="ç­›é€‰é€€å¸‚ç›¸å…³å…¬å‘Š")
    filter_parser.add_argument("stock_code", help="è‚¡ç¥¨ä»£ç ")
    filter_parser.add_argument("--limit", "-l", type=int, default=200, help="æŸ¥è¯¢å…¬å‘Šæ•°é‡ä¸Šé™")

    args = parser.parse_args()

    if args.command == "list-announcements":
        client = CNINFOClient()
        results = client.list_announcements(
            args.stock_code,
            keyword=args.keyword,
            sort=args.sort,
            limit=args.limit
        )
        print(json.dumps(results, ensure_ascii=False, indent=2))

    elif args.command == "download-pdf":
        client = CNINFOClient()
        success = client.download_pdf(args.url, args.output)
        if success:
            print(json.dumps({"success": True, "path": args.output}))
        else:
            print(json.dumps({"success": False, "error": "Download failed"}))
            sys.exit(1)

    elif args.command == "extract-text":
        text = extract_text_from_pdf(args.pdf_path, args.max_pages)
        if text:
            print(text)
        else:
            print("Failed to extract text", file=sys.stderr)
            sys.exit(1)

    elif args.command == "append-result":
        try:
            if args.file:
                with open(args.file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            else:
                data = json.loads(args.data)
        except json.JSONDecodeError as e:
            print(f"Invalid JSON: {e}", file=sys.stderr)
            sys.exit(1)
        except FileNotFoundError:
            print(f"File not found: {args.file}", file=sys.stderr)
            sys.exit(1)

        if isinstance(data, list):
            success = True
            for item in data:
                if not append_result_to_csv(args.csv, item):
                    success = False
        else:
            success = append_result_to_csv(args.csv, data)
            
        if success:
            print(json.dumps({"success": True}))
        else:
            sys.exit(1)

    elif args.command == "validate":
        try:
            if args.file:
                with open(args.file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            else:
                data = json.loads(args.data)
        except json.JSONDecodeError as e:
            print(f"Invalid JSON: {e}", file=sys.stderr)
            sys.exit(1)
        except FileNotFoundError:
            print(f"File not found: {args.file}", file=sys.stderr)
            sys.exit(1)

        if isinstance(data, list):
            results = []
            all_valid = True
            for item in data:
                res = validate_result(item)
                results.append(res)
                if not res["valid"]:
                    all_valid = False
            print(json.dumps(results, ensure_ascii=False, indent=2))
            if not all_valid:
                sys.exit(1)
        else:
            result = validate_result(data)
            print(json.dumps(result, ensure_ascii=False, indent=2))
            if not result["valid"]:
                sys.exit(1)

    elif args.command == "scan-risk":
        client = CNINFOClient()
        # è·å–æœ€è¿‘çš„å…¬å‘Š
        announcements = client.list_announcements(
            args.stock_code,
            keyword="",
            sort="desc",
            limit=args.days * 3  # å‡è®¾æ¯å¤©æœ€å¤š3ä¸ªå…¬å‘Š
        )
        
        # è¿‡æ»¤æœ€è¿‘Nå¤©çš„å…¬å‘Š
        from datetime import datetime, timedelta
        cutoff_date = (datetime.now() - timedelta(days=args.days)).strftime("%Y-%m-%d")
        recent = [a for a in announcements if a.get("date", "") >= cutoff_date]
        
        # æ‰«æé£é™©
        result = scan_delist_risk(recent)
        result["stock_code"] = args.stock_code
        result["scan_days"] = args.days
        result["announcement_count"] = len(recent)
        
        print(json.dumps(result, ensure_ascii=False, indent=2))
        
        # å¦‚æœæœ‰ç´§æ€¥æˆ–é«˜é£é™©ï¼Œè¿”å›é0é€€å‡ºç 
        if result["risk_level"] in ["CRITICAL", "HIGH"]:
            sys.exit(1)

    elif args.command == "filter-delist":
        # ç­›é€‰é€€å¸‚ç›¸å…³å…¬å‘Š
        client = CNINFOClient()
        announcements = client.list_announcements(
            args.stock_code,
            keyword="",
            sort="desc",
            limit=args.limit
        )
        
        # é€€å¸‚ç›¸å…³å…³é”®è¯
        delist_keywords = [
            "å¸æ”¶åˆå¹¶", "æ¢è‚¡", "ç»ˆæ­¢ä¸Šå¸‚", "æ‘˜ç‰Œ", "é€€å¸‚",
            "åœç‰Œ", "é¢„æ¡ˆ", "è¦çº¦æ”¶è´­", "ä¸»åŠ¨é€€å¸‚",
            "è§¦å‘é€€å¸‚", "é€€å¸‚æ•´ç†", "è‚¡ä¸œå¤§ä¼š.*å†³è®®"
        ]
        
        import re
        filtered = []
        for ann in announcements:
            title = ann.get("title", "")
            for kw in delist_keywords:
                if re.search(kw, title):
                    ann["matched_keyword"] = kw
                    filtered.append(ann)
                    break
        
        # è¾“å‡ºç»“æœ
        result = {
            "stock_code": args.stock_code,
            "total_announcements": len(announcements),
            "filtered_count": len(filtered),
            "announcements": filtered
        }
        print(json.dumps(result, ensure_ascii=False, indent=2))

    else:
        parser.print_help()


if __name__ == "__main__":
    main()
